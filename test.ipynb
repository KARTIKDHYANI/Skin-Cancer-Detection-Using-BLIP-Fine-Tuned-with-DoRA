{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/admincit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Required for tokenizing sentences for BLEU scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admincit/DoRA-LoRA/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Blip2ForConditionalGeneration(\n",
       "  (vision_model): Blip2VisionModel(\n",
       "    (embeddings): Blip2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): Blip2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): Blip2QFormerModel(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (encoder): Blip2QFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
       "  (language_model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTSdpaAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Blip2ForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load the model and processor from your saved directory\n",
    "processor = AutoProcessor.from_pretrained(\"./BLIP_finetuned_dora_r16/checkpoint_26\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"./BLIP_finetuned_dora_r16/checkpoint_26\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"Pranavkpba2000/skin_cancer_small_dataset\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "The detected disease is Squamous Cell Carcinoma (SCC), a type of skin cancer that often appears as\n"
     ]
    }
   ],
   "source": [
    "index=2000\n",
    "# prepare image for the model\n",
    "inputs = processor(images=dataset[index]['image'], return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=25)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE-L Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_rougeL\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, processor, dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Process image and prompt target\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m     target_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Replace with actual prompt in dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Generate a caption with the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, processor, dataset):\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    for item in dataset:\n",
    "        # Process image and prompt target\n",
    "        inputs = processor(images=item[\"image\"], return_tensors=\"pt\").to(device)\n",
    "        target_prompt = item[\"prompt\"]  # Replace with actual prompt in dataset\n",
    "\n",
    "        # Generate a caption with the model\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs)\n",
    "            generated_caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        reference = target_prompt.split()  # Convert to token list for BLEU\n",
    "        candidate = generated_caption.split()\n",
    "        bleu_score = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        rouge_result = rouge_scorer_obj.score(target_prompt, generated_caption)\n",
    "        rouge_scores[\"rouge1\"].append(rouge_result[\"rouge1\"].fmeasure)\n",
    "        rouge_scores[\"rouge2\"].append(rouge_result[\"rouge2\"].fmeasure)\n",
    "        rouge_scores[\"rougeL\"].append(rouge_result[\"rougeL\"].fmeasure)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n",
    "    avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n",
    "    avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n",
    "\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2 Score: {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L Score: {avg_rougeL:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_model(model, processor, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admincit/DoRA-LoRA/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"Pranavkpba2000/skin_cancer_small_dataset\", split=\"test\")\n",
    "\n",
    "# Assuming dataset is a list of dictionaries like:\n",
    "# dataset = [{\"image\": image_data, \"label\": 0}, {\"image\": image_data, \"label\": 1}, ...]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_images_per_class(dataset, num_images_per_class=1):\n",
    "    \"\"\"\n",
    "    Extracts a specified number of images from each class and creates a new dataset.\n",
    "\n",
    "    Args:\n",
    "    - dataset: Original dataset containing images and labels.\n",
    "    - num_images_per_class: The number of images to extract from each class.\n",
    "    \n",
    "    Returns:\n",
    "    - new_dataset: A new dataset with the extracted images and labels.\n",
    "    \"\"\"\n",
    "    # Dictionary to store images by class (labels 0-7)\n",
    "    class_to_images = defaultdict(list)\n",
    "\n",
    "    # Group images by their labels\n",
    "    for item in dataset:\n",
    "        class_to_images[item[\"label\"]].append(item)\n",
    "\n",
    "    # Create a new dataset with only 4 images per class (0 to 7)\n",
    "    new_dataset = []\n",
    "    for label in range(8):  # There are 8 labels (0 to 7)\n",
    "        # Get the images for the current label, and ensure we take at most 4\n",
    "        images_of_class = class_to_images[label][:num_images_per_class]\n",
    "\n",
    "        # Add these images to the new dataset\n",
    "        new_dataset.extend(images_of_class)\n",
    "\n",
    "    return new_dataset\n",
    "\n",
    "# Example usage:\n",
    "new_dataset = extract_images_per_class(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]\n",
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label: 0\n",
      "Reference Text: The detected disease is Actinic Keratosis (AK), a precancerous condition characterized by scaly, crusty patches of skin. It can potentially develop into skin cancer if not treated.\n",
      "Generated Text: The detected disease is Benign Keratosis (BKL), a non-cancerous skin growth that often appears as a wart or mole. These are usually harmless but may need monitoring.\n",
      "BLEU Score: 0.09992869870690119\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.2857142857142857, recall=0.2962962962962963, fmeasure=0.2909090909090909), 'rouge2': Score(precision=0.1111111111111111, recall=0.11538461538461539, fmeasure=0.11320754716981132), 'rougeL': Score(precision=0.25, recall=0.25925925925925924, fmeasure=0.2545454545454545)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 0.16654299046104756\n",
      "\n",
      "Image Label: 1\n",
      "Reference Text: The detected disease is Basal Cell Carcinoma (BCC), a common form of skin cancer that typically appears as a pearly or waxy bump. It is slow-growing and usually doesn't spread.\n",
      "Generated Text: The detected disease is Basal Cell Carcinoma (BCC), a common form of skin cancer that typically appears as a pearly or waxy bump. It is slow-growing and usually doesn't spread. It is slow-growing and usually doesn't spread. It spread. It spread. It spread. It is slow-growing and usually doesn't spread. It spread. It\n",
      "BLEU Score: 0.5531022352846267\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.5423728813559322, recall=1.0, fmeasure=0.7032967032967034), 'rouge2': Score(precision=0.5344827586206896, recall=1.0, fmeasure=0.6966292134831461), 'rougeL': Score(precision=0.5423728813559322, recall=1.0, fmeasure=0.7032967032967034)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 0.6333267151010527\n",
      "\n",
      "Image Label: 2\n",
      "Reference Text: The detected disease is Benign Keratosis (BKL), a non-cancerous skin growth that often appears as a wart or mole. These are usually harmless but may need monitoring.\n",
      "Generated Text: The detected disease is Benign Keratosis (BKL), a non-cancerous skin growth that often appears as a wart or mole. These are usually harmless but may need monitoring.\n",
      "BLEU Score: 1.0\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 1.0\n",
      "\n",
      "Image Label: 3\n",
      "Reference Text: The detected disease is Melanoma (MEL), a serious and aggressive form of skin cancer that often presents as a mole with irregular edges or different colors. It requires immediate attention.\n",
      "Generated Text: The detected disease is Melanoma (MEL), a serious and aggressive form of skin cancer that often presents as a mole with irregular edges or different colors. It requires immediate attention.\n",
      "BLEU Score: 1.0\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 1.0\n",
      "\n",
      "Image Label: 4\n",
      "Reference Text: The detected disease is Nevus (NV), commonly known as a mole, which can vary in color and size. Most are harmless, but changes in size, shape, or color may require evaluation.\n",
      "Generated Text: The detected disease is Benign Keratosis (BKL), a non-cancerous skin growth that often appears as a wart or mole. These are usually harmless but may need monitoring.\n",
      "BLEU Score: 0.1012302904375176\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.42857142857142855, recall=0.3870967741935484, fmeasure=0.4067796610169491), 'rouge2': Score(precision=0.18518518518518517, recall=0.16666666666666666, fmeasure=0.17543859649122806), 'rougeL': Score(precision=0.39285714285714285, recall=0.3548387096774194, fmeasure=0.3728813559322034)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 0.1706824748728047\n",
      "\n",
      "Image Label: 5\n",
      "Reference Text: The detected disease is Squamous Cell Carcinoma (SCC), a type of skin cancer that often appears as a firm, red nodule or scaly patch. It can spread if not treated early.\n",
      "Generated Text: The detected disease is Squamous Cell Carcinoma (SCC), a type of skin cancer that often appears as a firm, red nodule or scaly patch. It can spread if not treated early.\n",
      "BLEU Score: 1.0\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 1.0\n",
      "\n",
      "Image Label: 6\n",
      "Reference Text: The detected disease is Dermatofibroma (DF), a benign growth on the skin that is typically brown or tan. These are harmless and generally do not require treatment.\n",
      "Generated Text: The detected disease is Melanoma (MEL), a serious and aggressive form of skin cancer that often presents as a mole with irregular edges or different colors. It requires immediate attention.\n",
      "BLEU Score: 0.09518930981816905\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.3333333333333333, recall=0.37037037037037035, fmeasure=0.3508771929824561), 'rouge2': Score(precision=0.10344827586206896, recall=0.11538461538461539, fmeasure=0.10909090909090909), 'rougeL': Score(precision=0.3, recall=0.3333333333333333, fmeasure=0.3157894736842105)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 0.14902568504696256\n",
      "\n",
      "Image Label: 7\n",
      "Reference Text: The detected disease is Vascular Lesions (VASC), abnormal blood vessel growths that can appear as red or purple spots on the skin. They are generally harmless but may require treatment for cosmetic reasons.\n",
      "Generated Text: The detected disease is Vascular Lesions (VASC), abnormal blood vessel growths that can appear as red or purple spots on the skin. They are generally harmless but may require treatment for cosmetic reasons.\n",
      "BLEU Score: 1.0\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "CIDEr Score: 0.0\n",
      "METEOR Score: 1.0\n",
      "\n",
      "Average BLEU Score: 0.6062\n",
      "Average ROUGE Scores: {'rouge1': 0.71898283102565, 'rouge2': 0.6367957832793868, 'rougeL': 0.7058141234323214}\n",
      "Average CIDEr Score: 0.0000\n",
      "Average METEOR Score: 0.6399\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import Blip2ForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"./BLIP_finetuned_dora_r16/checkpoint_30\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"./BLIP_finetuned_dora_r16/checkpoint_30\")\n",
    "\n",
    "# Load validation dataset\n",
    "validation_dataset = new_dataset\n",
    "\n",
    "# Define label to prompt mapping for evaluation\n",
    "label_to_prompt = {\n",
    "    0: \"The detected disease is Actinic Keratosis (AK), a precancerous condition characterized by scaly, crusty patches of skin. It can potentially develop into skin cancer if not treated.\",\n",
    "    1: \"The detected disease is Basal Cell Carcinoma (BCC), a common form of skin cancer that typically appears as a pearly or waxy bump. It is slow-growing and usually doesn't spread.\",\n",
    "    2: \"The detected disease is Benign Keratosis (BKL), a non-cancerous skin growth that often appears as a wart or mole. These are usually harmless but may need monitoring.\",\n",
    "    3: \"The detected disease is Melanoma (MEL), a serious and aggressive form of skin cancer that often presents as a mole with irregular edges or different colors. It requires immediate attention.\",\n",
    "    4: \"The detected disease is Nevus (NV), commonly known as a mole, which can vary in color and size. Most are harmless, but changes in size, shape, or color may require evaluation.\",\n",
    "    5: \"The detected disease is Squamous Cell Carcinoma (SCC), a type of skin cancer that often appears as a firm, red nodule or scaly patch. It can spread if not treated early.\",\n",
    "    6: \"The detected disease is Dermatofibroma (DF), a benign growth on the skin that is typically brown or tan. These are harmless and generally do not require treatment.\",\n",
    "    7: \"The detected disease is Vascular Lesions (VASC), abnormal blood vessel growths that can appear as red or purple spots on the skin. They are generally harmless but may require treatment for cosmetic reasons.\"\n",
    "}\n",
    "\n",
    "# Initialize evaluators\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "cider_evaluator = Cider()\n",
    "meteor_evaluator = Meteor()\n",
    "\n",
    "# Function to evaluate BLEU, ROUGE, CIDEr, and METEOR scores on validation set\n",
    "def evaluate_metrics(model, processor, validation_dataset):\n",
    "    model.eval()\n",
    "    total_bleu_score = 0\n",
    "    total_rouge_score = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    total_cider_score = 0\n",
    "    total_meteor_score = 0\n",
    "    count = 0\n",
    "\n",
    "    for item in validation_dataset:\n",
    "        image = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Generate caption\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**inputs,max_length=80)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Reference prompt\n",
    "        reference_text = label_to_prompt[label]\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        reference_tokens = reference_text.split()\n",
    "        generated_tokens = generated_text.split()\n",
    "        bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "        total_bleu_score += bleu_score\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_instance.score(reference_text, generated_text)\n",
    "        for key in total_rouge_score:\n",
    "            total_rouge_score[key] += rouge_scores[key].fmeasure\n",
    "\n",
    "        # Calculate CIDEr score (CIDEr expects a list of dicts)\n",
    "        cider_score, _ = cider_evaluator.compute_score(\n",
    "            {0: [reference_text]}, {0: [generated_text]}\n",
    "        )\n",
    "        total_cider_score += cider_score\n",
    "\n",
    "        # Calculate METEOR score\n",
    "        meteor_score, _ = meteor_evaluator.compute_score(\n",
    "            {0: [reference_text]}, {0: [generated_text]}\n",
    "        )\n",
    "        total_meteor_score += meteor_score\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        print(f\"Image Label: {label}\")\n",
    "        print(f\"Reference Text: {reference_text}\")\n",
    "        print(f\"Generated Text: {generated_text}\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "        print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "        print(f\"CIDEr Score: {cider_score}\")\n",
    "        print(f\"METEOR Score: {meteor_score}\\n\")\n",
    "\n",
    "    # Calculate average scores\n",
    "    average_bleu_score = total_bleu_score / count\n",
    "    average_rouge_score = {k: v / count for k, v in total_rouge_score.items()}\n",
    "    average_cider_score = total_cider_score / count\n",
    "    average_meteor_score = total_meteor_score / count\n",
    "\n",
    "    print(f\"Average BLEU Score: {average_bleu_score:.4f}\")\n",
    "    print(f\"Average ROUGE Scores: {average_rouge_score}\")\n",
    "    print(f\"Average CIDEr Score: {average_cider_score:.4f}\")\n",
    "    print(f\"Average METEOR Score: {average_meteor_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"average_bleu_score\": average_bleu_score,\n",
    "        \"average_rouge_score\": average_rouge_score,\n",
    "        \"average_cider_score\": average_cider_score,\n",
    "        \"average_meteor_score\": average_meteor_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "average_metrics = evaluate_metrics(model, processor, validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.9.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.conda/lib/python3.11/site-packages (from gradio) (4.6.2.post1)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.5.2 (from gradio)\n",
      "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in ./.conda/lib/python3.11/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in ./.conda/lib/python3.11/site-packages (from gradio) (0.26.2)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.conda/lib/python3.11/site-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from gradio) (2.1.3)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.11/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in ./.conda/lib/python3.11/site-packages (from gradio) (11.0.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.conda/lib/python3.11/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.conda/lib/python3.11/site-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.11/site-packages (from gradio-client==1.5.2->gradio) (2024.9.0)\n",
      "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.2->gradio)\n",
      "  Downloading websockets-14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-5.9.0-py3-none-any.whl (57.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, websockets, uvicorn, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, mdurl, markupsafe, ffmpy, annotated-types, aiofiles, starlette, pydantic, markdown-it-py, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.9.0 gradio-client-1.5.2 markdown-it-py-3.0.0 markupsafe-2.1.5 mdurl-0.1.2 orjson-3.10.12 pydantic-2.10.3 pydantic-core-2.27.1 pydub-0.25.1 python-multipart-0.0.19 rich-13.9.4 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.41.3 tomlkit-0.13.2 typer-0.15.1 uvicorn-0.34.0 websockets-14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://42db03d7b500f2c76f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://42db03d7b500f2c76f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to process and generate caption\n",
    "def generate_caption(index):\n",
    "    # Load the image from the dataset (ensure compatibility with PIL.Image)\n",
    "    image = dataset[index]['image']\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    # Prepare image for the model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    # Generate caption\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=25)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Draw the caption on the image\n",
    "    img = image.convert(\"RGB\")  # Ensure RGB format\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Wrap text to fit within the image width\n",
    "    max_width = img.width - 20  # Allow for padding\n",
    "    words = generated_caption.split()\n",
    "    lines = []\n",
    "    line = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        test_line = f\"{line} {word}\" if line else word\n",
    "        test_width = draw.textbbox((0, 0), test_line, font=font)[2] - draw.textbbox((0, 0), test_line, font=font)[0]\n",
    "        if test_width <= max_width:\n",
    "            line = test_line\n",
    "        else:\n",
    "            lines.append(line)\n",
    "            line = word\n",
    "    if line:\n",
    "        lines.append(line)\n",
    "\n",
    "    # Calculate text height and position\n",
    "    line_height = draw.textbbox((0, 0), \"Test\", font=font)[3] - draw.textbbox((0, 0), \"Test\", font=font)[1]\n",
    "    text_height = line_height * len(lines)\n",
    "    text_position = (10, img.height - text_height - 10)  # Bottom padding\n",
    "\n",
    "    # Draw each line of text\n",
    "    for i, line in enumerate(lines):\n",
    "        draw.text((10, text_position[1] + i * line_height), line, fill=\"white\", font=font)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_caption,\n",
    "    inputs=gr.Number(label=\"Image Index\"),\n",
    "    outputs=gr.Image(label=\"Image with Caption\"),\n",
    "    title=\"Image Caption Generator\",\n",
    "    description=\"Enter an index to view an image with its generated caption.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dora_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
